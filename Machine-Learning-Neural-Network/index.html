<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/BlogPosting" >
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Machine Learning Neural Network.</title>
    <meta name="description" content="Technical blogs.">

    <!-- Google Authorship Markup -->
    <link rel="author" href="https://plus.google.com/+?rel=author">

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@">
    <meta name="twitter:title" content="Machine Learning Neural Network.">
    <meta name="twitter:description" content="Technical blogs.">
    
    <meta property="twitter:image:src" content="https://abyte.streamhttps://abyte.stream/assets/img/blog-image.png">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://abyte.streamhttps://abyte.stream/Machine-Learning-Neural-Network/">
    <meta property="og:title" content="Machine Learning Neural Network.">
    
    <meta property="og:image" content="https://abyte.streamhttps://abyte.stream/assets/img/blog-image.png">
    
    <meta property="og:description" content="Technical blogs.">
    <meta property="og:site_name" content="Sanjay Patel - Blogs">

    <!-- Social: Google+ / Schema.org  -->
    <meta itemprop="name" content="Machine Learning Neural Network."/>
    <meta itemprop="description" content="Technical blogs.">
    <meta itemprop="image" content="https://abyte.streamhttps://abyte.stream/assets/img/blog-image.png"/>

    <!-- Favicon -->
    <link rel="shortcut icon" href="https://abyte.stream/assets/img/icons/favicon.ico" type="image/x-icon" />
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!--amp page-->
    
        <link rel="amphtml" href="https://abyte.streamhttps://abyte.stream/amp/Machine-Learning-Neural-Network.html">
    
    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Sanjay Patel Blog">
    <meta name="msapplication-TileColor" content="#0562DC">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="">

    <link rel="stylesheet" href="https://abyte.stream/assets/css/main.css">
    <link rel="canonical" href="https://abyte.streamhttps://abyte.stream/Machine-Learning-Neural-Network/">
    <link rel="alternate" type="application/rss+xml" title="Sanjay Patel - Blogs" href="https://abyte.streamhttps://abyte.stream/feed.xml" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script src="/assets/js/lazysizes.min.js" async=""></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1067793-4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-1067793-4');
    </script>

</head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol></defs></svg>

        <header class="bar-header">
    <h2 class="logo">
        <a href="https://abyte.stream/"></a>
    </h2>
</header>
<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
      <li><a href="https://abyte.stream/">Home</a></li>
    
      <li><a href="https://abyte.stream/series">Series</a></li>
    
      <li><a href="https://abyte.stream/tags">Tags</a></li>
    
      <li><a href="https://abyte.stream/pretty-print/">Pretty Print</a></li>
    
      <li><a href="https://abyte.stream/sort-distinct/">Sort & Distinct</a></li>
    
    <li><a class="feed" href="https://abyte.stream/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

<header class="header-post" role="banner">
     <div class="content">
        
            <time itemprop="datePublished" datetime="2018-12-13T18:57:06-05:00" class="date">13 Dec 2018</time>
        
        <h1 class="post-title" itemprop="name">Machine Learning Neural Network.</h1>
        <p itemprop="description" class="subtitle"></p>
    </div>
</header>
        <section class="post">

            <section class="share">
            <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Ad Unit1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-6254849299387322"
     data-ad-slot="2070150131"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
            </section>
            <article role="article" id="post" class="post-content" itemprop="articleBody">
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>Neural network set of network with input, hidden and output neurons/nodes. Each neuron is <strong>activation function</strong> (for ex gradient descend) and each synapse/link have some weights. Every node maintains error metric and coefficient are adjusted to lower the error metric using back propagation. <strong>Bias</strong> are some constant values at neuron.</p>

<p><strong>Ok all above is fine but why?</strong><br />
<img src="/assets/2019-06-12-Machine-Learning-Basic-Maths22.JPG" alt="" class="lazyload" />
Let think of example, what number is there in below image. It is blurred not accurate but it is 2. Every human handwriting will create different image of 2 probably but human understand it is 2. How machine can know it is 2. Actually it doesn’t know it, it says the probability of this image being 2 is let say 90%. And we write the code which can take pixel of this image as input and generate possible results with probability. How much a pixel is lit can be i/p for next layer and affect how much next layer neuron are lit, in the end if algorithm is good it will lit the output neuron which is marked for 2 with some probability.</p>

<p><strong>activation function</strong> - Typically have Non-linear, continuos differentiable and fixed range.</p>

<p><strong>Loss or Cost Function</strong> - Defines accuracy of prediction with given neural network model.</p>

<p><strong>Optimization Algorithms</strong>- //TODO</p>
<h2 id="forward-propagation">Forward propagation</h2>
<p>Let say we have n layers and let’s pick two layer, each layer will have some activation function, Superscript number is layer number and subscript number is neuron number on that layer from top to bottom, <script type="math/tex">a_k^n</script> will n<sup>th</sup>  layer and k<sup>th</sup> node/neural at that level.</p>
<ul>
  <li><script type="math/tex">w^L</script> is weight at level L</li>
  <li><script type="math/tex">b^L</script> is bias at level L</li>
  <li><script type="math/tex">a^{L-1}</script> is X or input for level L and output of level L-1 which is activation function applied on z<sup>L</sup>.</li>
  <li>
    <script type="math/tex; mode=display">z^L = w^la^{L-1}+b^L</script>
  </li>
  <li>C is cost function. <script type="math/tex">C_0=(a^L-y)^2</script></li>
  <li>y is expected output</li>
  <li><script type="math/tex">\sigma'</script> is derivative of activation function wrt z<sup>L</sup></li>
</ul>

<p>So the formula to derive <script type="math/tex">a_k^n</script> will be</p>

<script type="math/tex; mode=display">a^1=\sigma(Wa^0+b)\\
\begin{bmatrix}a_1^n\\a_2^n\\..\\a_k^n\end{bmatrix}= \sigma\left(\begin{bmatrix}W\end{bmatrix}^{k_{n-1}\times k_{n}}\begin{bmatrix}a_1^{n-1}\\a_2^{n-1}\\..\\a_k^{n-1}\end{bmatrix}+\begin{bmatrix}B\end{bmatrix}^{{k_n}\times 1}\right)</script>

<h2 id="back-propagation">Back propagation</h2>
<p>Goal is to adjust the weights so that each overall cost is minimized.
Example of 1 node at each level.</p>

<p><img src="/assets/2019-06-12-Machine-Learning-Basic-Maths20.JPG" alt="" class="lazyload" />
Calculate change in C wrt to W<sub>L</sub>. <script type="math/tex">\frac{\delta C_0}{\delta w^L}=\frac{\delta z^L}{\delta w^L}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= a^{(L-1)}.\sigma'(z^L).2(a^L-y)</script>
For k number of training examples. <script type="math/tex">C_1,C_2..C_k</script> will be the cost function and nudging L level weight w<sub>L</sub> will be <script type="math/tex">\frac{\delta C}{\delta w^L}=\frac{1}{n}\sum_{k=0}^{n-1}\frac{\delta C_k}{\delta w^L}</script>
We saw weight now we need talk about changing b<sup>L</sup> and a<sup>L-1</sup>
<script type="math/tex">% <![CDATA[
\nabla C = \begin{bmatrix}\frac{\delta C}{\delta w^1}&\frac{\delta C}{\delta a^{b^1}}&\frac{\delta C}{\delta a^{L-1}}\\
\frac{\delta C}{\delta w^2}&\frac{\delta C}{\delta a^{b^2}}&\frac{\delta C}{\delta a^{L-2}}\\
...&...&...\\
\frac{\delta C}{\delta w^L}&\frac{\delta C}{\delta a^{b^L}}&\frac{\delta C}{\delta a^0}
\end{bmatrix} %]]></script>
So C wrt to <script type="math/tex">b^L</script> and <script type="math/tex">a^{L-1}</script> is as follows.
<script type="math/tex">\frac{\delta C_0}{\delta b^L}=\frac{\delta z^L}{\delta b^L}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= 1.\sigma'(z^L).2(a^L-y)\\\\
\frac{\delta C_0}{\delta a^{L-1}}=\frac{\delta z^L}{\delta a^{L-1}}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= w^L.\sigma'(z^L).2(a^L-y)</script>
Now let take k node at L-1 level and j nodes at L level. At this level <script type="math/tex">\nabla C</script> changes at seems complex but it is not. 
<script type="math/tex">\Delta C =\eta \nabla C</script> Where <script type="math/tex">\nabla c</script> is negative small change, gradient descend and <script type="math/tex">\eta</script> is learning rate or step size. Keeping too small will slow down and keeping big will overshoot. Later update w and b by these small changes.
<img src="/assets/2019-06-12-Machine-Learning-Basic-Maths21.JPG" alt="" class="lazyload" /></p>

<p>Commutatively we can say <script type="math/tex">\delta_j^L=\frac{\delta C}{\delta z^l_j}</script>. If this value big that mean changes at this z will make big impact to lower down the C rather then other lower derivative.</p>

<p><strong>stochastic gradient descent</strong> - If there are large training set, the learning might take time so we take a small set.</p>

<h3 id="activation-function">Activation Function</h3>
<p>There are few substantial activation function which we use most of the times, sigmoid, tanh(scaled sigmoid), relu, leaky ReLu. 
<strong>Step</strong> - On of off, 0 or 1, decide based on threshold. This is not analog so turning off few neuron completely will make them passive which we don’t want.</p>

<p><strong>Linear</strong> - Change in proportionate to w and <script type="math/tex">a^n</script>. If the all the layer have lennar function, then logically combined function will be linear and it will be dumb.</p>

<p><strong>Sigmoid</strong> - <script type="math/tex">f(x)=\frac{1}{1+e^{-2x}}</script>. It fits the a from range <script type="math/tex">[-\infty,+\infty]</script> to [0,1]. Gradient is result hungry if it is in between [-2,2] beyond this gradient is very slow and stops learning.</p>

<p><strong>tanh</strong> - <script type="math/tex">tanh(x)= 2sigmoid-1</script>, scales to [-1,1] beyond it gradient is slow.</p>

<p><strong>ReLu</strong> - A(x) = max(0,x). Activation value Less then 0 will be 0. Makes calculation faster but makes half network passive as they are always off.</p>

<p><strong>Leaky ReLu</strong> - Less then 0 is kept very small but not 0, y = 0.0x. Makes all nodes active. Solves ReLu problem. There other variant, <strong>ELU</strong>, less then 0 will be exponential <script type="math/tex">\alpha(e^z-1)</script></p>

<p>Thanks to 3Blue1Brown videos on neural network, makes visualization so easy.</p>
<iframe width="420" height="315" src="https://www.youtube.com/watch?v=tIeHLnjs5U8" frameborder="0" allowfullscreen=""></iframe>

<h3 id="increase-efficiency">Increase efficiency.</h3>
<p><strong>Cross Entropy Function</strong> - (Not same as probability distribution cross entropy, do not get confused). In place quadratic cost function (y-a)<sup>2</sup> use cross entropy function to calculate change required for weigh and biases. Learning rate doesn’t slow down as quadratic cost function.
<script type="math/tex">C=−1n∑x[ylna+(1−y)ln(1−a)]</script></p>

<p><strong>SoftMax and loglikelyhood</strong> - In place of 0 to 1 output probability on each output layer(sigmoid activation), if we say combined output layer probability should be 1(Softmax activation). Then the data is more comparable. Here cost function used is -log a, in case of output close to 1 less change is needed and in 0 more change is required.
 <script type="math/tex">\sum_j a_j^L=\frac{\sum_j e^{z_j^L}}{\sum_k e^{z_k^L}}\\
 C=-log\;a_y^L</script></p>

<p><strong>overfitting or overtraining</strong> - the epoch from where you dont see much learning after that.</p>

<p><strong>Bias &amp; Variance Tradeoff</strong> - Bias is difference between average predicted value to true value. High bias Pays very little attention to training set and over simplifies the model(<strong>underfitting</strong>). For ex. Model devices is linear function while the actual need was non linear function. Variance tells about the spread of the data. High variance over learn the training set and doesn’t generalize it (<strong>overfitting</strong>). It picks the outliers/noise and noise also in its knowledge which is overfiting of the data.  High parameter increase high variance and low parameter increase high bias, we have to trade off in selecting right parameter.
<script type="math/tex">{\displaystyle \operatorname {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}+\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}+\sigma ^{2}}\\
=(f-E[\hat f])^2+Var[y]+Var[\hat y]\\
=(f-E[\hat f])^2+E[\varepsilon^2]+E[(E[\hat f]-\hat f])^2]</script></p>

<p><strong>Detect underfit and overfit</strong> - Divide training data in training and test data and use test data to get the accuracy of model. 
<strong>Fixing underfit and overfit</strong></p>
<ul>
  <li>Cross-validation - Divide data in splits and train and test. For ex K-Fold, divide data in K and keep increasing the training set, keep decreasing the test set.</li>
  <li>Train with relevant data- More data can be good, but if it noisy it is issue so train with relevant data.</li>
  <li>Remove feature - Remove irrelevant features. Rubber duck debugging.</li>
  <li>Early stop - More training overfits the data sometime so know when to stop.</li>
  <li>Regularization - Make model simpler sometime, prune decision tree, dropout neural network, penalty parameter.</li>
  <li>Ensembling - Multiple model learn separately and combine them in the end to smooth it out. Bagging and boosting are example. Bagging start with complex model then smooths it out while boosting start with weak learner models and form a complex model.</li>
</ul>

<p><strong>Weight decay Regularization- L2 or Ridge Regularization</strong> - Add a extra term regularization term, <script type="math/tex">C=C_0+\frac{\lambda}{2n}\sum_ww^2\\
\frac{\delta C}{\delta w} = \frac{\lambda}{n}w</script>. It helped in overcoming overfitting issue and increasing accuracy, also saved us from local minima.</p>

<p><strong>Weight decay Regularization-L1 or Lasso Regularization (LAD - Least Absolute deviation)</strong> -</p>

<ol>
  <li>
    <p><strong>Sparsity</strong> - Defines how much element in vector or matrix are zero, more zero means more sparsity. L1 is more sparse.</p>
  </li>
  <li>
    <p><strong>Built-In feature Selection</strong> - Since L1 is sparse it brings down the wait of non used parameters which automatically bring more weighted parameters.</p>
  </li>
  <li>
    <p><strong>Dropout</strong> - We select set of hidden neuron and turn them off and repeat the machine learning for small batches, later average it out. It remove the overfitting of data as different network will pick different things and on average it will span out better.</p>
  </li>
</ol>

<ul>
  <li>Increase in training data increases the accuracy. We can increase training data by skewing image, rotating image etc.</li>
  <li>Initialize weight right, Normally we use normal/gaussian distribution in that case mean is 0 and SD is 1 but since these normally distributed weights will make z 0 and any change in weight will not impact cost hence learning will be very slow. Use <script type="math/tex">\frac{1}{\sqrt n}</script>.</li>
</ul>

<p><strong>Derive Hyper parameter fast</strong></p>
<ul>
  <li>Work with simple output prediction, such as 0,1 yes no etc.</li>
  <li>Start with simple network</li>
  <li>Take small batches.</li>
  <li>Adjust lambda in proportion to reduces batch size.</li>
  <li><script type="math/tex">\eta</script>, start with big and small see where it starts oscillating, it should not overshoot.</li>
  <li>Keep monitoring, stop early. no-improvement-in-ten etc.</li>
</ul>

<h1 id="neural-network-andrews-ng-way">Neural Network Andrews Ng way</h1>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network1.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network2.JPG" alt="" class="lazyload" />
Vectorization takes advantage of SIMD (Single instruction Multiple data)</p>
<h3 id="gradient-descend-vectorization">Gradient Descend vectorization</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network9.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network3.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network4.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network5.JPG" alt="" class="lazyload" /></p>

<ul>
  <li>In NN input layer is not counted as layer, so we start from <script type="math/tex">a^[0]</script> which is input layer. If you see 3 layers including input and output, it will be called 2 layer NN.</li>
</ul>

<p><strong>Multilayer neural network vectorized form</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network6.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network7.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network8.JPG" alt="" class="lazyload" /></p>

<p><strong>Derivative of activation function</strong>
<script type="math/tex">% <![CDATA[
sigmoid = g(z) = a \;then\; g'(z)= 1-a \\
tanh = g(z)=tanh(z) = a \;then\;g'(z) = 1-a^2\\
relu = g(z)=max(0,z) = a \;then\;g'(z) = \begin{cases} 0\;when\;z<0\\1\;when\;z>0\end{cases}\\
leaky relu = g(z)=max(0.01z,z) = a \;then\;g'(z) = \begin{cases} 0.01\;when\;z<0\\1\;when\;z>0\end{cases} %]]></script></p>

<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network10.JPG" alt="" class="lazyload" /></p>

<p><strong>Keep track of dimensions</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network11.JPG" alt="" class="lazyload" /></p>

<p><strong>Deep network calculations</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network12.JPG" alt="" class="lazyload" /></p>

<p><strong>Backward and forward propagation formulas</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network13.JPG" alt="" class="lazyload" /></p>

<h2 id="fine-tune-nn">Fine-tune NN</h2>
<p>Earlier ML used to have 70, 30 ratio to train and test set. Now a day due to big data the data set has increase so train set goes around 99% remaining is dev and test set. Dev test cross validation data for developer to find out right ML algorithm. Dev and test set should be from same distribution set, if not dev and test wouldn’t be same.</p>

<h3 id="bias-and-variance">Bias and Variance.</h3>
<p>Training error gives idea about bias, high error is high bias. Dev error -training error gives variance, high difference high variance. In NN we do not have tradeoff situation, as it was there in ML algorithms. NN provide different ways to reduce only bias or only variance.
Fix Bias : Increase neural network size by increasing layer or increasing nodes, Or train longer, Or choose better NN architecture.
Fix variance: More data or Regularize, or better NN architecture.</p>

<h3 id="regularization">Regularization</h3>
<p>L1 - Absolute value of weights
L2(weight decay) - Squared value of weights
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network14.JPG" alt="" class="lazyload" /></p>

<h3 id="dropout">Dropout</h3>
<p>Dropout, on off different nodes at different inputs tends to generalize the network. This is also one of the regularization technique.
<strong>Inverted Dropout</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network15.JPG" alt="" class="lazyload" /></p>

<h3 id="vanishing-and-exploding-gradient-descend">Vanishing and exploding Gradient descend.</h3>
<p>Many times gradient becomes to big or too small which just diverges the cost function. To overcome this issue there are multiple solution.
<strong>Random Weights initialization based on Activation function</strong>
Based on number of input node distribute weight to sum up to 1, which make each weight 1/n. Relu works better with 2 in place of 1.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network16.JPG" alt="" class="lazyload" /></p>

<p><strong>Gradient Checking</strong> - Sometime while coding we would want to know weather our gradients calculation are correct in code. To do so we merge all W’s, b’s,dW’s and db’s to single dimension array call θ and dθ. Later calculate approx dθ by nudging ε a bit. In the end compare the difference between these approx and model’s calculated derivatives.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network17.JPG" alt="" class="lazyload" /></p>

<p><strong>Grad check Do’s and Don’ts</strong></p>
<ul>
  <li>Do it only once, do not do it each training iterations.</li>
  <li>In case difference is big compare component wise, for ex <script type="math/tex">dW_L[i,j]</script> to  <script type="math/tex">dW_L[i,j]_{aprox}</script></li>
  <li>Doesn’t work with dropout.</li>
</ul>

<h3 id="optimization">Optimization.</h3>
<p><strong>Mini Batch gradient descend</strong> - Create batch, calculate and apply weight changes and iterate over all the batch. Repeat this till you get to minimum. It adds up noise in cost function curve. It is fastest as it usage mini batch and vectorization both.</p>

<p><strong>Stochastic G D</strong> - Mini batch size is 1. It is more noisy then mini batch. It doesn’t use vectorization much hence it is slow.</p>

<p><strong>Batch GD</strong> - Here batch size is m, full train dataset.</p>

<p><strong>Fit mini batch to CPU GPU memory to run faster</strong>, such 64, 128, 246,512, beyond this is rare for now.</p>

<p><strong>Exponential weighted Avg</strong> - Averaging current with previous values with some proportion to previous values.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network18.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network19.JPG" alt="" class="lazyload" /></p>

<p><strong>Bias Correction</strong> - In Exponential weighted average, initial average is biased as it starts from zero, to fix we use a different formula. 
<script type="math/tex">v_{t+1} = \frac{v_t}{1-\beta^t}</script></p>

<p><strong>Apply EWA to Gradient Descend</strong> - This somewhat resolves issue of noise due small batch size, also helps with the issue of overshooting with higher learning rate. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network20.JPG" alt="" class="lazyload" /></p>

<p><strong>RMS prop</strong> - Root mean square, same as momentum dampening.
learning rate alpha 
beta from momentum (usually 0.9) 
beta2 from RMSprop (usually 0.999) 
epsilon (usually 1e-8) 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network22.JPG" alt="" class="lazyload" /></p>

<p><strong>Adam Optimization</strong> - It is combination of both RMS and EWA.</p>
<ul>
  <li>There are other as well such, Nestrov accelerated gradient, Adagard, Adadelta</li>
</ul>

<p><strong>Learning rate decay</strong> - Once model start reaching to minimum vale the learning rate can be reduced so that it converges well. There are multiple formula’s to decay learning rate based on epoch. For ex. 
<script type="math/tex">\alpha=\frac{1}{1+decay\_rate*epcoh\_num}</script></p>

<h3 id="local-optima-problem">Local optima problem.</h3>
<p>Multidimensional space local optima is normally not a problem as dimension increase the chances of reaching local minima are very low. For ex 20000 feature will have probability 1/20000. So it is actually a plateaus, the thing put back on horse to sit on. It will always have other directions which can still minimize.</p>

<h3 id="batch-norm">Batch Norm</h3>
<p>Sometime it is good to normalize z’s or a’z of each layer.
We calculate tilda z, as normalized form of z, and rescale it with β and γ
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network23.JPG" alt="" class="lazyload" />
β is constant and can be eliminated as we already have rescaled the z.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network24.JPG" alt="" class="lazyload" />
Batch norm intuition is about, hidden layer don’t need to bother about changing inputs due to earlier layer learning, with z and  a norm hidden layer will learn based on a standard input, doesn’t whatever initial layers have. It make hidden layers independent of initial layers.</p>
<ul>
  <li>At test time, we use trained EWA of μ and σ value.</li>
</ul>

<h2 id="softmax-classification">Softmax Classification.</h2>
<p>When the output of NN has predefined classes, it becomes a problem of softmax where we calculate probability of classes. Hardmax just says 1 for one class and others as 0.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network25.JPG" alt="" class="lazyload" /></p>

<h3 id="single-number-evaluation-matrix-for-model">Single Number Evaluation matrix for Model.</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network26.JPG" alt="" class="lazyload" /></p>

<p><strong>F1 score</strong> is harmonic mean(reduces effect of outlier average), <strong>precision</strong>(% of actual cats out of model recognized cats) and <strong>recall</strong>(% of correctly recognized cat out of actual cats)</p>

<ul>
  <li>Precision is TPAP (TP/AP), recall is TPPP(TP/PP).</li>
  <li>Accuracy is how good model is predicting correct values.</li>
  <li>F1 harmonic mean of TP and FN, it is better model as accuracy requires same number of sample on postivie and negative, while F1 can work well with imbalance data also.</li>
  <li>ROC (Receiver Operating Characteristics) - Area under curve is AUC(Area Under The Curve). 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network88.JPG" alt="" class="lazyload" /></li>
</ul>

<h3 id="satisfying-and-optimizing-metric">Satisfying and optimizing metric</h3>
<p>Sometime there is good precision of model but take a lot time, some has low precision but takes less time. Create  Satisfying criteria, what is max time be bare to run and with that find model which has highest precision.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network27.JPG" alt="" class="lazyload" /></p>

<ul>
  <li>Dev and test set should be from same distribution.</li>
  <li>We can omit test set but not advised.</li>
  <li>For really unwanted predicted result, penalize those result to increase the error percentage.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network28.JPG" alt="" class="lazyload" /></li>
</ul>

<h2 id="bayess-error">Bayes’s Error</h2>
<p>It’s a upper limit defined by us before starting on any problem, We can’t make machine more accurate they Bayes’s error. For ex, faded image, even human give answer only 80% times right then machine can’t have 100% accuracy so we define an upper limit for machine learning and it is Baye’s error. Sometime It is same as human error but always above then human error.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network29.JPG" alt="" class="lazyload" /></p>

<p>Avoidable Bias is gap between human error and training error.</p>

<ul>
  <li>Carry out error analysis - In errors, classify errors and see which type of error is most and fix that.</li>
  <li>Random errors in training set is fine, but if it is systematic machine will learn on that error and create issues.</li>
</ul>

<h3 id="tuning-model-using">Tuning Model using.</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network30.JPG" alt="" class="lazyload" /></p>

<h3 id="transfer-learning">Transfer learning</h3>
<p>Sometime you can use existing model with different problem such as cat detection to radiology or speech recognition to wake work. In this you just add remove last layer and append one or more extra layer.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network31.JPG" alt="" class="lazyload" /></p>

<h3 id="multitask-learning">Multitask learning</h3>
<p>Autonomous driving one image can have multiple thing, signs, lights, pedestrian etc. All can stacked up in Y matrix and use bigger network to learn all thing together rather then one by one. It always works better then training individual neural network.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network32.JPG" alt="" class="lazyload" /></p>

<h3 id="end-to-end-deep-learning">End to end deep learning</h3>
<p>Big enough neural network with huge train data would suffice the for any complex x to y mapping. Some time when we less x to y mapping train data and we have more of intermediate data then we use component wise neural network model and here it works best, if you have huge data then end to end works best.
For ex, image to text detection, text to phrase detection, phrase to intent detection. This can be split into multiple small models or can be created in single big neural network, based on train set we decide which one to choose. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network33.JPG" alt="" class="lazyload" /></p>

<h1 id="convolution-network">Convolution network.</h1>
<p>For images, you have million pixels in your images these, that mean million feature, implementing this in normal neural network will be very inefficient, so we convolute(complex) neural net.</p>

<h3 id="edge-detection">Edge detection.</h3>
<p>Use 3X3 filter(Kernel) to find edges.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network34.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network35.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network36.JPG" alt="" class="lazyload" /></p>
<ul>
  <li>In above diagram you white to black is comes as white edge and black to white comes as dark edge, later you 30 and 10 that is because the edge has converted from white edge to back edge. If you transpose the filter matrix it becomes horizontal filter. Later image you see there are other filter which has some other properties, some are oriented filter(not vertical or horizontal but rotated to some degrees). Neural net learns to find best possible filter matrix.</li>
</ul>

<h3 id="padding">Padding</h3>
<p>Since output becomes smaller then input we pad input pixel with 0 so that output becomes same size of input. Filter are normally of odd sized so padding should be full number. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network37.JPG" alt="" class="lazyload" /></p>

<h3 id="strided-convolution">Strided Convolution.</h3>
<p>Jump more then 1 step in applying filter. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network38.JPG" alt="" class="lazyload" /></p>

<p><strong>Maths, Cross-Correlation vs Convolution</strong> - In maths all above work which we have done is cross-correlation, convolution involves one more step of filliping filter before applying.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network39.JPG" alt="" class="lazyload" /></p>

<h3 id="multiple-channelsthird-dimension-ex-rgb-and-multiple-filters-combinesex-horizontal-with-vertical-etc">Multiple channels(Third dimension ex RGB) and Multiple filters combines(ex Horizontal with vertical etc)</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network40.JPG" alt="" class="lazyload" /></p>

<h3 id="notations-for-one-layer-cnn">Notations for One layer CNN</h3>
<p>Here we discuss about only one layer, consider different filters operation produces a different output nodes, in single filter you have 3 dimension data which is actually a flattened weight on 3<em>3</em>3 size vector.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network41.JPG" alt="" class="lazyload" /></p>

<h3 id="type-of-cnn-layers">Type of CNN layers</h3>
<ul>
  <li>Convolution Layer - Which we saw above</li>
  <li>Pooling Layer (Reducer, no parameter to learn)</li>
  <li>Fully connected layer.</li>
</ul>

<h3 id="pooling-layer">Pooling layer</h3>
<p>These layer are the reducers, they reduce the size of each dimension by either max pooling (get max out of 1 block) or average (rarely used). It doesn’t have any hyperparameter hence no learning of variables here. This is said to be used to pick one particular feature from block(Max) or average of block feature.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network42.JPG" alt="" class="lazyload" /></p>

<h3 id="simple-cn-ex-lenet-5">Simple CN (Ex Lenet-5)</h3>
<ul>
  <li>Normally the pattern is CL and PL few more CL and PL then some FC layers.</li>
  <li>H and W decreases while C(third dimension) increases</li>
  <li>PL has 0 parameter to lean, CL has less parameters, FCL has most parameters to learn.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network43.JPG" alt="" class="lazyload" /></li>
</ul>

<h3 id="why-convolution">Why convolution?</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network44.JPG" alt="" class="lazyload" /></p>

<h3 id="case-study">Case study.</h3>
<p>Here we have three different CNN papers.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network45.JPG" alt="" class="lazyload" /></p>

<h3 id="res-net-">Res Net .</h3>
<p>Residue net, it keeps the residue of previous layer by giving shortcut input to higher layers.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network46.JPG" alt="" class="lazyload" /></p>

<p><strong>Why ResNets work,</strong> It fights back to diminishing issue due to Weight Decay L2 regularization. If Weight diminishes in next layer, the input from previous layer are already mapped to third layer.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network47.JPG" alt="" class="lazyload" /></p>

<p>Resnet has two type of blocks. First when the shortcut doesn’t match the output dimension(convolution block) and when it matches (identity block)
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network87.JPG" alt="" class="lazyload" /></p>

<h3 id="1-by-1-network">1 by 1 network</h3>
<p>Polling can be used to reduce height and width, how about depth or channels, it is decrease by 1 by 1 filter.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network48.JPG" alt="" class="lazyload" /></p>

<h3 id="inception-layer">Inception Layer</h3>
<p>It usage 1 by 1 network concept to reduce the computation cost. Here is the comparison.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network50.JPG" alt="" class="lazyload" />
Inception layer have multiple layer together and stacked up later. Combining multiple inception layer can work better also adding fully connected layer</p>

<h3 id="transfer-learning-1">Transfer learning</h3>
<p>Take related model from github and replace last softmax layer with your classification softmax layer when you have very less data to train, in this case you will freeze calculation of previous layers and use pre calculated values , if you have more data to train, take github data start with that model and train whole model.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network51.JPG" alt="" class="lazyload" /></p>

<h2 id="object-detection">Object detection</h2>
<p>Object detection is finding out object inside a picture, it more advance then the detecting what image is out or image classification.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network52.JPG" alt="" class="lazyload" /></p>

<h3 id="notation-and-loss-calculation-for-object-detection">Notation and Loss calculation for object detection.</h3>
<p>bx, by are center for object, c1, c2,c3 will either have 0 or 1 which class it belong to, and Px is probability(mostly 1) of that object.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network53.JPG" alt="" class="lazyload" /></p>

<h3 id="landmark-detection-multiple-points-or-shapes">LandMark detection (Multiple points or shapes)</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network54.JPG" alt="" class="lazyload" /></p>

<h3 id="sliding-window-with-convolution">Sliding window with Convolution.</h3>
<p>Sliding window you will have to crop many pictures, but convolution NN automatically does that with step as stride numbers.So all cropped images has run simultaneously and shared the computations
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network55.JPG" alt="" class="lazyload" /></p>

<h3 id="yolo-algorithm">YOLO Algorithm</h3>
<p>It runs very fast, it divides the image in grid and learn on weather the object is falls in box or not and it height/width with respect to box size. With these input data, YOLO starts to learn picking things.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network56.JPG" alt="" class="lazyload" />
<strong>What you should remember:</strong></p>
<ul>
  <li>YOLO is a state-of-the-art object detection model that is fast and accurate</li>
  <li>It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.</li>
  <li>The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.</li>
  <li>You filter through all the boxes using non-max suppression. Specifically:
    <ul>
      <li>Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li>
      <li>Intersection over Union (IoU) thresholding to eliminate overlapping boxes
Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.</li>
    </ul>
  </li>
</ul>

<h3 id="iou">IoU</h3>
<p><img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network57.JPG" alt="" class="lazyload" /></p>

<h3 id="non-max-suppression">Non Max suppression.</h3>
<p>Model tend to detect multiple boxes for same object, then non max suppression picks one with highest probability and with this box find IoU with other boxes, if IoU of highest Px box and other box is grater then 0.5, then we discard box.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network58.JPG" alt="" class="lazyload" /></p>

<h3 id="multiple-object-over-lap">Multiple object over lap.</h3>
<p>Create anchor boxes and increase y vector by maximum overlapping object that model should detect. Lets say y was 8 and we changed it to 8*2=16, it can detect two shape anchor boxes. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network59.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network60.JPG" alt="" class="lazyload" /></p>

<h2 id="face-recognition">Face Recognition</h2>
<p>Face verification is comparing image with another image, while recognition is finding out which person is this among thousands person.
<strong>One Shot Learning</strong> - These day system takes only one image of subject and starts recognizing it. So if you have only 1 image of subject and have to recognize its other images then we use One shot learning technique. It mainly work on distance between two given images. Same image distance will be 0 while different image distance will be more. Once is network is trained to pick different feature from face, it can be used for new subject image as well. Calculate feature vector of new person and save in DB for first time, later use this feature vector to judge weather provided image is of this person or not. 
α is margin, how much different can be accepted as same image. 
<strong>Hard triplet</strong> - For image distance learning, if you provide image subject(A), positive image(P) and negative (N), A and N are different people, then the distance will be definitely high and it will not help machine to learn. N should be of different person but should be little bit similar to A it make it hard to learn, which improves model learning. 
<strong>Triplet Loss</strong> - Loss function to learn on triplets of image A,P,N. Later used in gradient descend to learn the NN model.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network61.JPG" alt="" class="lazyload" /></p>

<h3 id="face-verification-and-binary-classification">Face verification and Binary classification</h3>
<p>Above approach was good but we can have alternative as, above model is tweaked little bit and used to identify person by comparing the saved feature vectors(calculated and save in DB) against new image feature vector to identify if new user is from database or not. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network62.JPG" alt="" class="lazyload" /></p>

<h3 id="neural-style-transfer">Neural Style Transfer.</h3>
<p>Use style of another image(S) to subject image(C) to create new image G. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network63.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network64.JPG" alt="" class="lazyload" /></p>

<h2 id="sequence-model">Sequence Model</h2>
<p>It is used when you have sequence of data, such as language translation, NLP, Music creation etc, Video filtration. Here the output can word to word match(Tx = Ty, length of input equal length of output) or even sometime it can’t be equal also. Moreover, to predict next block output you might need input data of previous block as well(such NLP sentence meaning is formed when you consider previous word also). Here we use RNN.</p>

<h3 id="recurring-neural-network">Recurring Neural Network</h3>
<p>Previous block A is fed to next block NN calculation.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network65.JPG" alt="" class="lazyload" /></p>

<p>Here, W<sub>aa</sub> and W<sub>ax</sub> is written side by side in one matrix and marked as W<sub>a</sub>, same with a and x. In backprop you try to reduce sum of loss at each output.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network66.JPG" alt="" class="lazyload" /></p>

<p><strong>Types on RNN</strong>
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network67.JPG" alt="" class="lazyload" /></p>

<p><strong>Language Modeling and sequence generation</strong>
When model is trained on some test sentences, it start to make prediction on next words based on previous words.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network68.JPG" alt="" class="lazyload" /></p>
<ul>
  <li>
    <p>In character level model the charters are passed in place of words and vocab is formed from character. This model doesn’t work better then word model also it take more computation time and power.</p>
  </li>
  <li>Vanishing gradient problem, In deep NN initial layer impact is very less on last layer.</li>
  <li>Exploding gradient, In calculation if weights starts increasing exponentially we use gradient clipping to limit the exploding weights. Use of proper weights also fixed the issue.</li>
</ul>

<p>To remember old words and overcome vanishing gradient descend, we have following options.</p>
<ul>
  <li>GRU</li>
  <li>LSTM</li>
  <li>Switch to Relu from sigmoid function.</li>
</ul>

<h3 id="gru---gated-recurrent-unit">GRU - Gated recurrent Unit</h3>
<p>Add another weight W<sub>u</sub> with existing W<sub>a</sub>(represented here as W<sub>c</sub>)
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network69.JPG" alt="" class="lazyload" /></p>

<h3 id="lstm---better-then-gru">LSTM - Better then GRU</h3>
<p>It has few more gates to learn. Update, forget and output gates.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network70.JPG" alt="" class="lazyload" /></p>

<h3 id="brnn---bidirectional-rnn">BRNN - Bidirectional RNN</h3>
<p>Once model learning is complete, BRNN requires full sentence to predict the words between with it’s probabibility. It is not much used as it requires full sentence, there much more complex models in place to which work on realtime data.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network71.JPG" alt="" class="lazyload" /></p>
<h3 id="deep-rnn">Deep RNN.</h3>
<p>RNN with multiple RNN layer is deep RNN.</p>

<h3 id="word-embedding">Word embedding,</h3>
<p>We use feature matrix in place of word vector, so the dimension is reduced from 10k to 300 in our example, if 10k were words and 300 are feature in vector. Feature vector helps us to relate two different words by analogy like gender, royalty and it can derive similarity as man, women then king will have queen , country capital etc.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network72.JPG" alt="" class="lazyload" /></p>

<h3 id="cosine-similarity-vs-squared-difference">Cosine similarity vs Squared difference.</h3>
<p>Cosine between two vector tells about how similar they are, big value is they are more similar, cos(0)=1 and small value they are not at all similar cos(90)=1, while squared difference tell different they are, big value they are different small value they are similar. Check the image above. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network73.JPG" alt="" class="lazyload" /></p>

<h3 id="derive-word-embeding">Derive Word embeding.</h3>
<p>THere is matrices E, formed formed of 300 feature for 10k word. If you multiple one hot vector of word to E you will get e (word embedding), also you can stack 10k e(word embedding) vector to form E.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network74.JPG" alt="" class="lazyload" /></p>

<h3 id="finding-target-based-on-context-word">Finding target based on Context word.</h3>
<p><strong>Skip Gram Model</strong>
Context and target word are picked randomly from sentence.
Here we have E(Combined embed vector) and θ(Softmax) to learn
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network75.JPG" alt="" class="lazyload" /></p>

<h3 id="negative-sampling">Negative sampling</h3>
<p>Rather then iterating over 10k words to predict using softmax and update 10K e (E) and θ, lets take only k sample for ex 5 negative and 1 positive and update E based on only these training set. How to select negative sample is, either pick randomly (issue common word will show up more) or pick using word’s frequency weight. P(w) and f(w).
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network76.JPG" alt="" class="lazyload" /></p>

<h3 id="sentiment-classification">Sentiment Classification.</h3>
<p>Tune E which gets e(feature) value and average it up to find out sentiment of sentence, but sometimes one negative word in starting changes the complete meaning of sentence, in this case average doesn’t work. RNN is good model to remember outcomes from previous words as well.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network77.JPG" alt="" class="lazyload" /></p>

<h3 id="biased-learning-issue">Biased learning issue.</h3>
<p>Sometime due to more data, such as babysitter are female, nurses are female creates biased learning for NN. But this is not actually the case in real world man can be female and nurses to remove these biases, we pass learned NN to neutralization algorithm which neutralize feature vector for nurse and baby sitter in gender direction. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network78.JPG" alt="" class="lazyload" />
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network86.JPG" alt="" class="lazyload" /></p>

<h2 id="sequnece-to-sequence-generation">Sequnece to Sequence generation</h2>
<p>When we have stream and we want to generate stream, such as machine translation, image captioning etc.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network79.JPG" alt="" class="lazyload" /></p>

<h3 id="conditional-language-model">Conditional language model.</h3>
<p>Language generation model(generate sequence of word based on one input) look similar to sequence to sequence generation model. But S2S model give a feature vector output of given input and passed to language generation model which looks more conditional. We have two option at each word generation pick highest probability output word (greedy) or average out all generated word probability(more optimal).
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network80.JPG" alt="" class="lazyload" /></p>

<h3 id="beam-search">Beam Search</h3>
<p>Here we store top B probability words at each output, and based on conditional probability we keep making prediction of next words, in case of B=3, first word will have 3 possible outcome among 10k, next will have 3*10K and only three will be selected and rest discarded. At reaching EOF, the sentence having max probability will be the result. 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network81.JPG" alt="" class="lazyload" /></p>

<h3 id="refinement-to-beam-search">Refinement to Beam Search</h3>
<p>Normalization- Since conditional probabilities can be very small and cause undeflow errors, in place of product of probability we use sum of log probability, sum of log probability can be big then we normalize by dividing with count of vocabulary words we have or some percentage of that α
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network82.JPG" alt="" class="lazyload" /></p>

<h3 id="error-analysis">Error analysis</h3>
<p>Who is at fault RNN or Beam window, you can find this out by comparing probability of p(y*) vs p(<script type="math/tex">\hat y</script>). 
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network83.JPG" alt="" class="lazyload" /></p>

<h3 id="attention-model">Attention Model.</h3>
<p><strong>Bleu Score</strong>- Bilingual evaluation understudy. 1 value to validate good translation is working. In this we compare the machine generated sentence with human given reference sentence and see how same they by checking the common word occupance. 
If the sentence is too large Blue score will drop, to maintain it we want to keep translating rather than waiting for EOF. Just like human we read some word and translate, it is call giving attention to few nearby words.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network84.JPG" alt="" class="lazyload" /></p>

<p>Spectrogram</p>
<h3 id="speech-recognition--trigger-word-detection">Speech recognition &amp; Trigger word Detection.</h3>
<p>For both we use the same above attention model, but rather than having text as input we have speech which is frequency sliced basis of time, let’s say 10 second audio sliced to 10k times and fed in machine to print the sentence. 
Trigger word are trained to give 1 only when the trigger word occurred.
<img src="/assets/2019-06-12-2020-06-12-Machine-Learning-Neural-Network85.JPG" alt="" class="lazyload" /></p>

            </article>

            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20https://abyte.stream/Machine-Learning-Neural-Network/%20via%20&#64;&hashtags=Machine Learning,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=https://abyte.stream/Machine-Learning-Neural-Network/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=https://abyte.stream/Machine-Learning-Neural-Network/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
            <section class="share">
                <div class="fb-comments" data-href="https://abyte.stream/Machine-Learning-Neural-Network/"></div>
            </section>
            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/assets/img/blog-author.jpg" alt="">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="https://plus.google.com/+/posts">Sanjay Patel</a>
        </h3>
        <p class="desc">Developer at IBM</p>
        <p><a itemprop="email" class="email" href="mailto:sanjaypatel2525@yahoo.com">sanjaypatel2525@yahoo.com</a></p>
        <!--<p><a itemprop="github" class="github" href="https://github.com/">github.com/</a></p>-->
    </div>
</section>

            <footer>
    <p>Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a></p>
</footer>
<script src="https://abyte.stream/assets/js/main.js"></script>
            <div id="fb-root"></div>
        </section>
    </body>
    
    <script>(function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_GB/all.js#xfbml=1&appId=207343132703435";
        fjs.parentNode.insertBefore(js, fjs);
        }(document, "script", "facebook-jssdk"));</script>
</html>
