<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/BlogPosting" >
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Machine Learning Neural Network.</title>
    <meta name="description" content="Technical blogs.">

    <!-- Google Authorship Markup -->
    <link rel="author" href="https://plus.google.com/+?rel=author">

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@">
    <meta name="twitter:title" content="Machine Learning Neural Network.">
    <meta name="twitter:description" content="Technical blogs.">
    
    <meta property="twitter:image:src" content="https://abyte.stream/assets/img/blog-image.png">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://abyte.stream/Machine-Learning-Neural-Network/">
    <meta property="og:title" content="Machine Learning Neural Network.">
    
    <meta property="og:image" content="https://abyte.stream/assets/img/blog-image.png">
    
    <meta property="og:description" content="Technical blogs.">
    <meta property="og:site_name" content="Sanjay Patel - Blogs">

    <!-- Social: Google+ / Schema.org  -->
    <meta itemprop="name" content="Machine Learning Neural Network."/>
    <meta itemprop="description" content="Technical blogs.">
    <meta itemprop="image" content="https://abyte.stream/assets/img/blog-image.png"/>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/assets/img/icons/favicon.ico" type="image/x-icon" />
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!--amp page-->
    
        <link rel="amphtml" href="https://abyte.stream/amp/Machine-Learning-Neural-Network.html">
    
    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Sanjay Patel Blog">
    <meta name="msapplication-TileColor" content="#0562DC">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://abyte.stream/Machine-Learning-Neural-Network/">
    <link rel="alternate" type="application/rss+xml" title="Sanjay Patel - Blogs" href="https://abyte.stream/feed.xml" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1067793-4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-1067793-4');
    </script>

</head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol></defs></svg>

        <header class="bar-header">
    <h1 class="logo">
        <a href="/"></a>
    </h1>
</header>
<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
      <li><a href="https://abyte.stream/">Home</a></li>
    
      <li><a href="https://abyte.stream/series">Series</a></li>
    
      <li><a href="https://abyte.stream/tags">Tags</a></li>
    
      <li><a href="https://abyte.stream/pretty-print/">Pretty Print</a></li>
    
      <li><a href="https://abyte.stream/sort-distinct/">Sort & Distinct</a></li>
    
    <li><a class="feed" href="https://abyte.stream/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

<header class="header-post" role="banner">
     <div class="content">
        
            <time itemprop="datePublished" datetime="2018-12-13T18:57:06-05:00" class="date">13 Dec 2018</time>
        
        <h1 class="post-title" itemprop="name">Machine Learning Neural Network.</h1>
        <p itemprop="description" class="subtitle"></p>
    </div>
</header>
        <section class="post">

            <section class="share">
            <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Ad Unit1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-6254849299387322"
     data-ad-slot="2070150131"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
            </section>
            <article role="article" id="post" class="post-content" itemprop="articleBody">
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>Neural network set of network with input, hidden and output neurons/nodes. Each neuron is <strong>activation function</strong> (for ex gradient descend) and each synapse/link have some weights. Every node maintains error metric and coefficient are adjusted to lower the error metric using back propagation. <strong>Bias</strong> are some constant values at neuron.</p>

<p><strong>Ok all above is fine but why?</strong><br />
<img src="/assets/2019-06-12-Machine-Learning-Basic-Maths22.JPG" alt="" />
Let think of example, what number is there in below image. It is blurred not accurate but it is 2. Every human handwriting will create different image of 2 probably but human understand it is 2. How machine can know it is 2. Actually it doesn’t know it, it says the probability of this image being 2 is let say 90%. And we write the code which can take pixel of this image as input and generate possible results with probability. How much a pixel is lit can be i/p for next layer and affect how much next layer neuron are lit, in the end if algorithm is good it will lit the output neuron which is marked for 2 with some probability.</p>

<p><strong>activation function</strong> - Typically have Non-linear, continuos differentiable and fixed range.</p>

<p><strong>Loss or Cost Function</strong> - Defines accracy of prediction with given neural network model.</p>

<p><strong>Optimization Algorithms</strong>- //TODO</p>
<h2 id="forward-propagation">Forward propagation</h2>
<p>Let say we have n layers and let’s pick two layer, each layer will have some activation function, Superscript number is layer number and subscript number is neuron number on that layer from top to bottom, <script type="math/tex">a_k^n</script> will n<sup>th</sup>  layer and k<sup>th</sup> node/neural at that level.</p>
<ul>
  <li><script type="math/tex">w^L</script> is weight at level L</li>
  <li><script type="math/tex">b^L</script> is bias at level L</li>
  <li><script type="math/tex">a^{L-1}</script> is X or input for level L and output of level L-1 which is activation function applied on z<sup>L</sup>.</li>
  <li>
    <script type="math/tex; mode=display">z^L = w^la^{L-1}+b^L</script>
  </li>
  <li>C is cost function. <script type="math/tex">C_0=(a^L-y)^2</script></li>
  <li>y is expected output</li>
  <li><script type="math/tex">\sigma'</script> is derivative of activation function wrt z<sup>L</sup></li>
</ul>

<p>So the formula to derive <script type="math/tex">a_k^n</script> will be</p>

<script type="math/tex; mode=display">a^1=\sigma(Wa^0+b)\\
\begin{bmatrix}a_1^n\\a_2^n\\..\\a_k^n\end{bmatrix}= \sigma\left(\begin{bmatrix}W\end{bmatrix}^{k_{n-1}\times k_{n}}\begin{bmatrix}a_1^{n-1}\\a_2^{n-1}\\..\\a_k^{n-1}\end{bmatrix}+\begin{bmatrix}B\end{bmatrix}^{{k_n}\times 1}\right)</script>

<p>##Back propogation
Goal is to adjust the weights so that each overall cost is minimized.
Example of 1 node at each level.</p>

<p><img src="/assets/2019-06-12-Machine-Learning-Basic-Maths20.JPG" alt="" />
Calculate change in C wrt to W<sub>L</sub>. <script type="math/tex">\frac{\delta C_0}{\delta w^L}=\frac{\delta z^L}{\delta w^L}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= a^{(L-1)}.\sigma'(z^L).2(a^L-y)</script>
For k number of training examples. <script type="math/tex">C_1,C_2..C_k</script> will be the cost function and nudging L level weight w<sub>L</sub> will be <script type="math/tex">\frac{\delta C}{\delta w^L}=\frac{1}{n}\sum_{k=0}^{n-1}\frac{\delta C_k}{\delta w^L}</script>
We saw weight now we need talk about changing b<sup>L</sup> and a<sup>L-1</sup>
<script type="math/tex">% <![CDATA[
\nabla C = \begin{bmatrix}\frac{\delta C}{\delta w^1}&\frac{\delta C}{\delta a^{b^1}}&\frac{\delta C}{\delta a^{L-1}}\\
\frac{\delta C}{\delta w^2}&\frac{\delta C}{\delta a^{b^2}}&\frac{\delta C}{\delta a^{L-2}}\\
...&...&...\\
\frac{\delta C}{\delta w^L}&\frac{\delta C}{\delta a^{b^L}}&\frac{\delta C}{\delta a^0}
\end{bmatrix} %]]></script>
So C wrt to <script type="math/tex">b^L</script> and <script type="math/tex">a^{L-1}</script> is as follows.
<script type="math/tex">\frac{\delta C_0}{\delta b^L}=\frac{\delta z^L}{\delta b^L}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= 1.\sigma'(z^L).2(a^L-y)\\\\
\frac{\delta C_0}{\delta a^{L-1}}=\frac{\delta z^L}{\delta a^{L-1}}\frac{\delta a_L}{\delta z^L}\frac{\delta C_0}{\delta a^L}\\
= w^L.\sigma'(z^L).2(a^L-y)</script>
Now let take k node at L-1 level and j nodes at L level. At this level <script type="math/tex">\nabla C</script> changes at seems complex but it is not. 
<script type="math/tex">\Delta C =\eta \nabla C</script> Where <script type="math/tex">\nabla c</script> is negative small change, gradient descend and <script type="math/tex">\eta</script> is learning rate or step size. Keeping too small will slow down and keeping big will overshoot. Later update w and b by these small changes.
<img src="/assets/2019-06-12-Machine-Learning-Basic-Maths21.JPG" alt="" /></p>

<p>Commuatively we can say <script type="math/tex">\delta_j^L=\frac{\delta C}{\delta z^l_j}</script>. If this value big that mean changes at this z will make big impact to lower down the C rather then other lower derivative.</p>

<p><strong>stochastic gradient descent</strong> - If there are large training set, the learning might take time so we take a small set.</p>

<h3 id="activation-function">Activation Function</h3>
<p>There are few substantial activation function which we use most of the times, sigmoid, tanh(scaled sigmoid), relu, leaky ReLu. 
<strong>Step</strong> - On of off, 0 or 1, decide based on threshold. This is not analog so turning off few neuron completely will make them passive which we don’t want.</p>

<p><strong>Linear</strong> - Change in proportionate to w and <script type="math/tex">a^n</script>. If the all the layer have lennar function, then logically combined function will be linear and it will be dumb.</p>

<p><strong>Sigmoid</strong> - <script type="math/tex">f(x)=\frac{1}{1+e^{-2x}}</script>. It fits the a from range <script type="math/tex">[-\infty,+\infty]</script> to [0,1]. Gradient is result hungry if it is in between [-2,2] beyond this gradient is very slow and stops learning.</p>

<p><strong>tanh</strong> - <script type="math/tex">tanh(x)= 2sigmoid-1</script>, scales to [-1,1] beyond it gradient is slow.</p>

<p><strong>ReLu</strong> - A(x) = max(0,x). Activation value Less then 0 will be 0. Makes calculation faster but makes half network passive as they are always off.</p>

<p><strong>Leaky ReLu</strong> - Less then 0 is kept very small but not 0, y = 0.0x. Makes all nodes active. Solves ReLu problem. There other variant, <strong>ELU</strong>, less then 0 will be exponential <script type="math/tex">\alpha(e^z-1)</script></p>

<p>Thanks to 3Blue1Brown videos on neural network, makes visualization so easy.</p>
<iframe width="420" height="315" src="https://www.youtube.com/watch?v=tIeHLnjs5U8" frameborder="0" allowfullscreen=""></iframe>

<h3 id="increase-efficiency">Increase efficiency.</h3>
<p><strong>Cross Entropy Function</strong> - (Not same as probability distribution cross entropy, do not get confused). In place quardratic cost function (y-a)<sup>2</sup> use cross entropy function to calculate change required for weigh and biases. Learning rate doesn’t slow down as quadratic cost function.
<script type="math/tex">C=−1n∑x[ylna+(1−y)ln(1−a)]</script></p>

<p><strong>SoftMax and loglikelyhood</strong> - In place of 0 to 1 output probability on each output layer(sigmoid activation), if we say combined output layer probability should be 1(Softmax activation). Then the data is more comparable. Here cost function used is -log a, in case of output close to 1 less change is needed and in 0 more change is required.
 <script type="math/tex">\sum_j a_j^L=\frac{\sum_j e^{z_j^L}}{\sum_k e^{z_k^L}}\\
 C=-log\;a_y^L</script></p>

<p><strong>overfitting or overtraining</strong> - the epoch from where you dont see much learning after that.</p>

<p><strong>Bias &amp; Variance Tradeoff</strong> - Bias is difference between average predicted value to true value. High bias Pays very little attention to training set and oversimplfies the model(<strong>underfitting</strong>). For ex. Model devices is lenear function while the actula need was non lenear function. Variance tells about the spread of the data. High variance over learn the training set and doesn’t generalize it (<strong>overfitting</strong>). It picks the outliers/noise and noise also in its knowledge which is overfiting of the data.  High parameter increase high variance and low parameter increase high bias, we have to trade off in selecting right parameter.
<script type="math/tex">{\displaystyle \operatorname {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}+\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}+\sigma ^{2}}\\
=(f-E[\hat f])^2+Var[y]+Var[\hat y]\\
=(f-E[\hat f])^2+E[\varepsilon^2]+E[(E[\hat f]-\hat f])^2]</script></p>

<p><strong>Detect underfit and overfit</strong> - Devide training data in training and test data and use test data to get the accuracy of model. 
<strong>Fixing underfit and overfit</strong></p>
<ul>
  <li>Cross-validation - Divide data in splits and train and test. For ex K-Fold, divide data in K and keep increasing the training set, keep decreasing the test set.</li>
  <li>Train with relevant data- More data can be good, but if it noisy it is issue so train with relevant data.</li>
  <li>Remove feature - Remove irrelevant features. Rubber duck debugging.</li>
  <li>Early stop - More training overfits the data sometime so know when to stop.</li>
  <li>Regularization - Make model simpler sometime, prune decision tree, dropout neural netwrok, penalty paramerter.</li>
  <li>Ensembling - Multiple model learn separately and cobmbine them in the end to smooth it out. Bagging and boosting are example. Bagging start with complex moddle then smooths it out while boosting start with weak learner models and form a complex model.</li>
</ul>

<p><strong>Weight decay Regulazrization- L2 or Ridge Regulazrization</strong> - Add a extra term regularization term, <script type="math/tex">C=C_0+\frac{\lambda}{2n}\sum_ww^2\\
\frac{\delta C}{\delta w} = \frac{\lambda}{n}w</script>. It helped in overcoming overfitting issue and increasing accruracy, also saved us from local minima.</p>

<p><strong>Weight decay Regulazrization-L1 or Lasso Regulazrization (LAD - Least Absolute deviation)</strong> - Since</p>

<p><strong>Sparsity</strong> - Defines how much element in vector or matrix are zero, more zero means more sparsity. L1 is more sparse.</p>

<p><strong>Built-In feature Selection</strong> - Since L1 is sparse it brings down the wait of non used parameters which automatically bring more weighted parameters.</p>

<p><strong>Dropout</strong> - We select set of hidden neuron and turn them off and repeat the machine learning for small batches, later average it out. It remove the overfitting of data as different network will pick different things and on average it will span out better.</p>

<ul>
  <li>Increase in training data increases the accuracy. We can increase training data by skewing image, rotating image etc.</li>
  <li>Initialize weight right, Normally we use normal/gaussian distribution in that case mean is 0 and SD is 1 but since these normally distributed weights will make z 0 and any change in weight will not impact cost hence learning will be very slow. Use <script type="math/tex">\frac{1}{\sqrt n}</script>.</li>
</ul>

<p><strong>Derive Hyper parameter fast</strong></p>
<ul>
  <li>Work with simple output prediction, such as 0,1 yes no etc.</li>
  <li>Start with simple network</li>
  <li>Take small batches.</li>
  <li>Adjust lambda in proportion to reduces batch size.</li>
  <li><script type="math/tex">\eta</script>, start with big and small see where it starts oscillating, it should not overshoot.</li>
  <li>Keep monitoring, stop early. no-improvement-in-ten etc.</li>
</ul>

            </article>

            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20https://abyte.stream/Machine-Learning-Neural-Network/%20via%20&#64;&hashtags="
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=https://abyte.stream/Machine-Learning-Neural-Network/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=https://abyte.stream/Machine-Learning-Neural-Network/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
            <section class="share">
                <div class="fb-comments" data-href="https://abyte.stream/Machine-Learning-Neural-Network/"></div>
            </section>
            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/assets/img/blog-author.jpg" alt="">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="https://plus.google.com/+/posts">Sanjay Patel</a>
        </h3>
        <p class="desc">Developer at IBM</p>
        <p><a itemprop="email" class="email" href="mailto:sanjaypatel2525@yahoo.com">sanjaypatel2525@yahoo.com</a></p>
        <!--<p><a itemprop="github" class="github" href="https://github.com/">github.com/</a></p>-->
    </div>
</section>

            <footer>
    <p>Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a></p>
</footer>
<script src="/assets/js/main.js"></script>
            <div id="fb-root"></div>
        </section>
    </body>
    
    <script>(function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_GB/all.js#xfbml=1&appId=207343132703435";
        fjs.parentNode.insertBefore(js, fjs);
        }(document, "script", "facebook-jssdk"));</script>
</html>
